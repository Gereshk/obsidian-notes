
# WSTG-INFO-01: Conduct Search Engine Recon for Info Leakage
**SUMMARY**: Search engines work by using bots to *crawl* billions of web pages searching for embedded links and sitemaps. Web pages can use a special file called `robot.txt` to blacklist pages and prevent them from being fetched by search engines.
>	See: [How Search Engines Work](https://developers.google.com/search/docs/fundamentals/how-search-works?hl=en&visit_id=638260579591177855-1863182502&rd=1)




> [!Resources]
> - [WSTG-INFO-01](https://github.com/OWASP/wstg/blob/master/document/4-Web_Application_Security_Testing/01-Information_Gathering/01-Conduct_Search_Engine_Discovery_Reconnaissance_for_Information_Leakage.md)
> - [Google: How Search Engines Work](https://developers.google.com/search/docs/fundamentals/how-search-works?hl=en&visit_id=638260579591177855-1863182502&rd=1)
> - 